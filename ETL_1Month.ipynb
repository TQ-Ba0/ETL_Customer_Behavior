{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <center> Project ETL behavior data customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd \n",
    "import os\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "from pyspark.sql.functions import lit\n",
    "import pyspark.sql.functions as sf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"8g\").config('spark.jars.packages','mysql:mysql-connector-java:8.0.17').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect HDFS and get file name data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URI           = spark._jvm.java.net.URI\n",
    "Path          = spark._jvm.org.apache.hadoop.fs.Path\n",
    "FileSystem    = spark._jvm.org.apache.hadoop.fs.FileSystem\n",
    "Configuration = spark._jvm.org.apache.hadoop.conf.Configuration\n",
    "fs = FileSystem.get(URI(\"hdfs://localhost:9900\"), Configuration())\n",
    "status = fs.listStatus(Path('/user/log_content'))\n",
    "Path_link=[]\n",
    "for fileStatus in status:\n",
    "    Path_link.append(fileStatus.getPath().getName())\n",
    "path_HDFS='hdfs://localhost:9900//user//log_content//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20220401.json',\n",
       " '20220402.json',\n",
       " '20220403.json',\n",
       " '20220404.json',\n",
       " '20220405.json',\n",
       " '20220406.json',\n",
       " '20220407.json',\n",
       " '20220408.json',\n",
       " '20220409.json',\n",
       " '20220410.json',\n",
       " '20220411.json',\n",
       " '20220412.json',\n",
       " '20220413.json',\n",
       " '20220414.json',\n",
       " '20220415.json',\n",
       " '20220416.json',\n",
       " '20220417.json',\n",
       " '20220418.json',\n",
       " '20220419.json',\n",
       " '20220420.json',\n",
       " '20220421.json',\n",
       " '20220422.json',\n",
       " '20220423.json',\n",
       " '20220424.json',\n",
       " '20220425.json',\n",
       " '20220426.json',\n",
       " '20220427.json',\n",
       " '20220428.json',\n",
       " '20220429.json',\n",
       " '20220430.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL 1 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ETL_1day(pathHDFS,filename):\n",
    "      Data=spark.read.json(pathHDFS+filename)\n",
    "      New_Data=Data.select('_source.*')\n",
    "      New_Data=New_Data.drop('Mac')\n",
    "      New_Data=New_Data.withColumn('Category',when((col(\"AppName\") == 'CHANNEL') | (col(\"AppName\") =='DSHD')| (col(\"AppName\") =='KPLUS')| (col(\"AppName\") =='KPlus'), \"Truyền Hình\")\n",
    "            .when((col(\"AppName\") == 'VOD') | (col(\"AppName\") =='FIMS_RES')| (col(\"AppName\") =='BHD_RES')| \n",
    "                  (col(\"AppName\") =='VOD_RES')| (col(\"AppName\") =='FIMS')| (col(\"AppName\") =='BHD')| (col(\"AppName\") =='DANET'), \"Phim Truyện\")\n",
    "            .when((col(\"AppName\") == 'RELAX'), \"Giải Trí\")\n",
    "            .when((col(\"AppName\") == 'CHILD'), \"Thiếu Nhi\")\n",
    "            .when((col(\"AppName\") == 'SPORT'), \"Thể Thao\")\n",
    "            .otherwise(\"Error\"))\n",
    "      New_Data=New_Data.select('Contract','Category','TotalDuration')\n",
    "      New_Data=New_Data.filter(New_Data.Category != 'Error')\n",
    "      New_Data=New_Data.groupBy('Contract','category').sum()\n",
    "      New_Data=New_Data.withColumnRenamed('sum(TotalDuration)','TotalDuration')\n",
    "      return New_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+\n",
      "| Contract|   category|TotalDuration|\n",
      "+---------+-----------+-------------+\n",
      "|DNH014998|Phim Truyện|         3365|\n",
      "|HND486882|Phim Truyện|         5545|\n",
      "|HUFD07189|Truyền Hình|         2264|\n",
      "|HDFD36288|Truyền Hình|        11904|\n",
      "|CTFD04401|Truyền Hình|        55881|\n",
      "|HNH954607|Phim Truyện|        13115|\n",
      "|HNH855959|Truyền Hình|          327|\n",
      "|SGH034683|Truyền Hình|        82195|\n",
      "|NTFD35330|Truyền Hình|        19139|\n",
      "|NTFD48198|Phim Truyện|        55202|\n",
      "|HNH443856|Truyền Hình|         7687|\n",
      "|NAFD05338|Truyền Hình|        81934|\n",
      "|LCFD20510|Phim Truyện|        10852|\n",
      "|QNFD29007|Truyền Hình|        82705|\n",
      "|SGH569599|Truyền Hình|        18769|\n",
      "|SGH701752|Truyền Hình|        31028|\n",
      "|HNH712164|Phim Truyện|         6106|\n",
      "|PYFD01920|Truyền Hình|        82664|\n",
      "|HTFD13716|Truyền Hình|        18759|\n",
      "|SGJ039473|Truyền Hình|        18695|\n",
      "+---------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ETL_1day(path_HDFS,Path_link[0]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create columns and ETL 30 day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat TotalDuration column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Total_Duration(df):\n",
    "    cols_list=df.columns[1:]\n",
    "    expression = '+'.join(cols_list)\n",
    "    Total_data=df.withColumn('TotalDuration',expr(expression))\n",
    "    Total_data=Total_data.select('Contract','TotalDuration')\n",
    "    return Total_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat Most_Watch column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MostWatch(df):\n",
    "        Most_watch=df.withColumnRenamed('RelaxDuration','Relax') \\\n",
    "            .withColumnRenamed('MovieDuration','Movie') \\\n",
    "            .withColumnRenamed('ChildDuration','Child') \\\n",
    "            .withColumnRenamed('SportDuration','Sport') \\\n",
    "            .withColumnRenamed('TvDuration','Tv') \n",
    "        Most_watch=Most_watch.withColumn(\n",
    "                                \"max\",sf.greatest(*[sf.col(x) for x in Most_watch.columns[1:]]))\\\n",
    "                             .withColumn(\n",
    "                                'Most_watch',eval(\"sf.when\" + \".when\".join([\"(sf.col('\" + c + \"') == sf.col('max'), sf.lit('\" + c + \"'))\" for c in Most_watch.columns])))\n",
    "        Most_watch=Most_watch.withColumn('Most_watch', when(col('max')== 0, 'No').otherwise(Most_watch.Most_watch))\n",
    "        Most_watch=Most_watch.select('Contract','Most_watch')\n",
    "        return Most_watch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tase column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tase(df):\n",
    "    Tase=df.withColumn('RelaxDuration',when(df.RelaxDuration != 0,'Relax').otherwise(None)) \\\n",
    "            .withColumn('MovieDuration',when(df.MovieDuration != 0,'Movie').otherwise(None)) \\\n",
    "            .withColumn('ChildDuration',when(df.ChildDuration != 0,'Child').otherwise(None)) \\\n",
    "            .withColumn('SportDuration',when(df.SportDuration != 0,'Sport').otherwise(None)) \\\n",
    "            .withColumn('TvDuration',when(df.TvDuration != 0,'Tv').otherwise(\"\"))  \n",
    "    Tase=Tase.withColumn('Tase',concat_ws('_',*[i for i in Tase.columns[1:]]))\n",
    "    Tase=Tase.withColumn('Tase',when(Tase.Tase=='','No').otherwise(Tase.Tase))\n",
    "    Tase=Tase.select('Contract','Tase')\n",
    "    return Tase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create Activeness clomun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Actieness(df):\n",
    "    Count_Active=df.groupBy('Contract').count()\n",
    "    Count_Active=Count_Active.withColumnRenamed('count','Activeness')\n",
    "    return Count_Active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL 30 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ETL_30_day():\n",
    "    Result1=ETL_1day(path_HDFS,Path_link[0])\n",
    "    Count_US=Result1.select('Contract')\n",
    "    Count_US=Count_US.distinct()\n",
    "    print('Job Finished {}'.format(Path_link[0]))\n",
    "    for i in Path_link[1:]:\n",
    "        Result2=ETL_1day(path_HDFS,i)\n",
    "        Result1=Result1.union(Result2)\n",
    "        Count_US_2=Result2.select('Contract')\n",
    "        Count_US_2=Count_US_2.distinct()\n",
    "        Count_US=Count_US.union(Count_US_2)\n",
    "        print('Job Finished {}'.format(i))\n",
    "        # Result1 = Result1.cache()\n",
    "    Activeness=Actieness(Count_US)\n",
    "    Result1=Result1.groupBy('Contract','Category').sum()\n",
    "    Result1=Result1.withColumnRenamed('sum(TotalDuration)','TotalDuration')\n",
    "    KQ=Result1.groupBy('Contract').pivot('Category').sum('TotalDuration')\n",
    "    KQ=KQ.withColumnRenamed('Giải Trí','RelaxDuration') \\\n",
    "                .withColumnRenamed('Phim Truyện','MovieDuration') \\\n",
    "                .withColumnRenamed('Thiếu Nhi','ChildDuration') \\\n",
    "                .withColumnRenamed('Thể Thao','SportDuration') \\\n",
    "                .withColumnRenamed('Truyền Hình','TvDuration') \n",
    "    KQ=KQ.fillna(0)\n",
    "    TotalDuration=Total_Duration(KQ)\n",
    "    Most_watch=MostWatch(KQ)\n",
    "    Tase_Data=Tase(KQ)\n",
    "    KQ=KQ.join(TotalDuration,'Contract','inner')\n",
    "    KQ=KQ.join(Most_watch,'Contract','inner')\n",
    "    KQ=KQ.join(Tase_Data,'Contract','inner')\n",
    "    KQ=KQ.join(Activeness,'Contract','inner')\n",
    "    KQ=KQ.withColumn('Date',sf.lit('2023-09-14'))\n",
    "    KQ.repartition(1).write.csv('E:\\\\CV\\\\DoAn1\\\\ETL_1_month',header=True)\n",
    "    print('job Finished succesfull')\n",
    "    return KQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Finished 20220401.json\n",
      "Job Finished 20220402.json\n",
      "Job Finished 20220403.json\n",
      "Job Finished 20220404.json\n",
      "Job Finished 20220405.json\n",
      "Job Finished 20220406.json\n",
      "Job Finished 20220407.json\n",
      "Job Finished 20220408.json\n",
      "Job Finished 20220409.json\n",
      "Job Finished 20220410.json\n",
      "Job Finished 20220411.json\n",
      "Job Finished 20220412.json\n",
      "Job Finished 20220413.json\n",
      "Job Finished 20220414.json\n",
      "Job Finished 20220415.json\n",
      "Job Finished 20220416.json\n",
      "Job Finished 20220417.json\n",
      "Job Finished 20220418.json\n",
      "Job Finished 20220419.json\n",
      "Job Finished 20220420.json\n",
      "Job Finished 20220421.json\n",
      "Job Finished 20220422.json\n",
      "Job Finished 20220423.json\n",
      "Job Finished 20220424.json\n",
      "Job Finished 20220425.json\n",
      "Job Finished 20220426.json\n",
      "Job Finished 20220427.json\n",
      "Job Finished 20220428.json\n",
      "Job Finished 20220429.json\n",
      "Job Finished 20220430.json\n",
      "job Finished succesfull\n"
     ]
    }
   ],
   "source": [
    "KQ=ETL_30_day()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push code to MySQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_MYSQL=KQ.limit(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'jdbc:mysql://' + 'localhost' + ':' + '3307' +'/'+'DataEngineer'\n",
    "driver = \"com.mysql.cj.jdbc.Driver\"\n",
    "user='root'\n",
    "password='1234'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_MYSQL.write.format('jdbc').option('url',url).option('driver',driver).option('dbtable','Customer_Behavior').option('user',user).option('password',password).mode('append').save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
